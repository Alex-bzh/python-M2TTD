{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9963a116-f067-4597-b69e-997a00adb26e",
   "metadata": {},
   "source": [
    "# Le partitionnement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d64df-3a47-4165-97b2-bd817b7b82b8",
   "metadata": {},
   "source": [
    "Le partitionnement (*clustering*) est une technique courante des statistiques multivariées pour effectuer des tâches de regroupement entre variables afin de révéler une structure sous-jacente. Il s’agit d’une méthode exploratoire qui aide à la classification des données en regroupant les individus dans des ensembles cohérents où la variance intra-groupes est minimisée quand la variance inter-groupes est, elle, maximisée.\n",
    "\n",
    "Importons les modules qui seront nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec46f4e-5f5a-4ba9-8d47-c36c90cab5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71b3cd-e546-4549-a4b5-68565e8042d0",
   "metadata": {},
   "source": [
    "## Une matrice de dissimilarité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c0949-3a61-46b7-b0ca-67adc6d3e081",
   "metadata": {},
   "source": [
    "### Avec des variables catégorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c9f95-bc15-4c22-b667-6c2119e3efd4",
   "metadata": {},
   "source": [
    "Prenons les réponses de cinq étudiant·es à un test comportant dix questions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333da78-df81-4bba-971d-a624fcb0794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_students = 5\n",
    "n_questions = 10\n",
    "\n",
    "data = [\n",
    "    [\n",
    "        random.choice(['A', 'B', 'C', 'D'])\n",
    "        for _ in range(n_questions)\n",
    "    ]\n",
    "    for _ in range(n_students)\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, index=[f\"Student {i+1}\" for i in range(n_students)], columns=[f'Q{i+1}' for i in range(0, n_questions)])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db398a-4716-4632-ac57-7e8ad2291116",
   "metadata": {},
   "source": [
    "Et faisons la comparaison deux à deux pour comptabiliser le nombre de fois où leurs réponses divergent. Enfin, normalisons en divisant le résultat par le nombre de questions afin d’obtenir un score entre 0 et 1 où 0 correspond à des étudiant·es aux réponses similaires et 1 des étudiant·es qui n’auront jamais répondu pareil :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1cace7-bedd-44dd-bee6-fdda64c750b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = df.values\n",
    "\n",
    "# calculate differences between rows\n",
    "row_diffs = (data_array[:, None] != data_array).sum(axis=2)\n",
    "\n",
    "# normalize\n",
    "dissimilarity_matrix = row_diffs / n_questions\n",
    "\n",
    "# matrix to a DataFrame\n",
    "dissimilarity_df = pd.DataFrame(dissimilarity_matrix, index=df.index, columns=df.index)\n",
    "\n",
    "display(dissimilarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d181c10-5acd-4cdf-9898-f2490510d347",
   "metadata": {},
   "source": [
    "Il est à présent facile d’identifier les paires d’étudiant·es dont les réponses se ressemblent le plus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb731e8-69fd-4074-8ba9-d701646ea996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum score > 0 to exclude pairs consisting of the same student\n",
    "min_score = dissimilarity_df[dissimilarity_df > 0].min().min()\n",
    "\n",
    "# all the pairs concerned\n",
    "clusters = dissimilarity_df[dissimilarity_df == min_score].stack().index.tolist()\n",
    "\n",
    "display(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57b48c-fe8e-426a-8f6b-3f2217a7f9ed",
   "metadata": {},
   "source": [
    "### Avec des variables numériques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ccb44-9eeb-4a77-b1fd-c91b6e26ebe5",
   "metadata": {},
   "source": [
    "Dans l’exemple précédent, les variables enregistraient des données catégorielles. Si maintenant nous prenons l’exemple d’une dizaine de textes avec des scores sur 20 dans cinq catégories :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a687d-42b0-41ab-992e-279f43365f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 texts with a score on 5 categories\n",
    "n_texts = 10\n",
    "categories = [\"Sciences\", \"Politique\", \"Littérature\", \"Journalisme\", \"Philosophie\"]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=np.random.randint(0, 21, size=(n_texts, len(categories))),\n",
    "    index=[f\"Text {i + 1}\" for i in range(0, n_texts)],\n",
    "    columns=categories\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c3e2d0-3e4e-43d5-af28-5f14d41307b3",
   "metadata": {},
   "source": [
    "Il n’est plus question ici de repérer les catégories où les textes ont obtenu des scores différents, aussi la première étape consiste à calculer une matrice de corrélation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a43442-2eb1-4196-94a4-9e188123cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "display(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd5f10-c543-4379-a05f-992823737c92",
   "metadata": {},
   "source": [
    "Cette matrice ressort des coefficients variant de -1 à 1 pour exprimer la corrélation entre chaque paire de variables. Pour la transformer en une matrice de dissimilarité, il suffit de calculer l’inverse de la corrélation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9655004-9bfe-4f8c-bdba-959dfe6596f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix = 1 / correlation_matrix\n",
    "\n",
    "display(dissimilarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff77d7-b270-47fd-8601-8623f1aca0cb",
   "metadata": {},
   "source": [
    "Une formule alternative consiste à calculer plutôt l’opposé de la corrélation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b34555-16fa-4b89-83ef-fffa013cfc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix = 1 - correlation_matrix\n",
    "\n",
    "display(dissimilarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07722506-b655-41b8-8e04-8c60ccc47d42",
   "metadata": {},
   "source": [
    "Puis à normaliser afin d’obtenir des scores dans l’intervalle $[0,1]$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3a0c6-b893-469f-aef0-d309fcd9e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dissimilarity_matrix / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d409b-8815-43c2-af0f-56542e4e08d4",
   "metadata": {},
   "source": [
    "De là, nous pouvons effectuer des prédictions sur les *clusters* formés entre les catégories. Peut-être la littérature et la philosophie sont-elles liées par leur coefficient de dissimilarité et, comme la philosophie et la politique sont elles-mêmes reliées, pourrions-nous en conclure que les trois disciplines forment un groupe.\n",
    "\n",
    "**Remarque :** les données sont générées aléatoirement aussi les groupes seront-ils toujours différents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d4f99-3233-4ca0-81d3-29504803770e",
   "metadata": {},
   "source": [
    "## Une matrice de distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0db63-49fd-4466-a5e0-e3a09d7d38cf",
   "metadata": {},
   "source": [
    "Bien souvent, lorsque l’on calcule la dissimilarité entre des vecteurs, on calcule la distance euclidienne :\n",
    "\n",
    "$$\n",
    "d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477ee91-61b2-4df5-8588-78ee1442dbf4",
   "metadata": {},
   "source": [
    "### Calcul de la distance euclidienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab24a1-7e13-4c13-b646-5a9f934e60f3",
   "metadata": {},
   "source": [
    "La fonction peut s’interpréter en Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8e720-6fd0-4ac9-927c-1ad2fd165722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(*, a:list, b:list) -> float:\n",
    "    \"\"\"Euclidean distance between two vectors.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    a -- first vector\n",
    "    b -- second vector\n",
    "    \"\"\"\n",
    "    # difference between indices\n",
    "    coords = [\n",
    "        (x - y) ** 2\n",
    "        for x, y in zip(a, b)\n",
    "    ]\n",
    "    # distance = square root of the sum of coords\n",
    "    return sum(coords) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9767a9-ed24-4f33-b6cc-0eb87e4b617a",
   "metadata": {},
   "source": [
    "Pour l’appliquer à notre jeu de données, on va d’abord générer une matrice de distances nulle avant de la remplir en se servant de la symétrie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785b2e3-da44-4e0b-aef1-b8f21134a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a null matrix\n",
    "pairwise_distances = np.zeros((n_texts, n_texts))\n",
    "\n",
    "for i in range(n_texts):\n",
    "    for j in range(i, n_texts):  # start at 'i' to avoid calculating b-a pair if a-b already stored\n",
    "        dist = euclidean_distance(a=df.iloc[i].values.tolist(), b=df.iloc[j].values.tolist())\n",
    "        pairwise_distances[i, j] = dist\n",
    "        pairwise_distances[j, i] = dist  # matrix is symetric\n",
    "\n",
    "display(pairwise_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7791b6c8-4cb5-4683-9f73-833cad08b442",
   "metadata": {},
   "source": [
    "### Calcul avec *Numpy*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de5d1b-5b53-44fd-babe-f3cbd69ddfbc",
   "metadata": {},
   "source": [
    "Calculer la distance euclidienne entre deux vecteurs *a* et *b* revient à calculer la norme du vecteur différence $a - b$.\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{v}\\| = \\sqrt{\\sum_{i=1}^{n} v_i^2}\n",
    "$$\n",
    "\n",
    "Or, la fonction `.linalg.norm()` de *Numpy* permet d’obtenir directement la norme d’un vecteur ; aussi pouvons-nous nous passer de la fonction `euclidean_distance()` définie plus haut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e5447-1ebf-47f3-8cae-d61705872f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a null matrix\n",
    "pairwise_distances = np.zeros((n_texts, n_texts))\n",
    "\n",
    "for i in range(n_texts):\n",
    "    # find vector a\n",
    "    vector_a = df.iloc[i].values\n",
    "    for j in range(i, n_texts):\n",
    "        # find vector b\n",
    "        vector_b = df.iloc[j].values\n",
    "        dist = np.linalg.norm(vector_a - vector_b)\n",
    "        pairwise_distances[i, j] = dist\n",
    "        pairwise_distances[j, i] = dist\n",
    "\n",
    "display(pairwise_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221450a-ee13-4496-878b-d4515e1c69f7",
   "metadata": {},
   "source": [
    "### Une bibliothèque scientifique spécialisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a173b-6cb4-4e06-ad14-e494b983a56b",
   "metadata": {},
   "source": [
    "La bilbiothèque *Scipy* fournit des méthodes spécialisées pour le calcul scientifique. Utilisons les fonctions `pdist` et `squareform` du module `spatial.distance` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d469d09-4d66-4448-b6c0-efb90895d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean matrix\n",
    "distances = pdist(df.values, metric='euclidean')\n",
    "\n",
    "# to a square matrix\n",
    "distance_matrix = squareform(distances)\n",
    "\n",
    "# to a dataframe\n",
    "distance_df = pd.DataFrame(distance_matrix, index=df.index, columns=df.index)\n",
    "\n",
    "display(distance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0410a6-1809-40c3-9b78-971e2420fa03",
   "metadata": {},
   "source": [
    "## Le regroupement hiérarchique ascendant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faabf20-b2fd-4195-8b0d-deb040eaee15",
   "metadata": {},
   "source": [
    "Jusqu’ici, les techniques de partitionnement proposées étaient plutôt simples et élémentaires : elles permettaient de dégager des appariements, tout au plus des regroupements de trois éléments. L’objectif à présent est de présenter une méthode plus systématique qui va, à chaque étape, effectuer des regroupements deux à deux jusqu’à ce qu’il ne reste plus qu’un seul groupe constitué de l’ensemble des autres.\n",
    "\n",
    "On peut matérialiser cette méthode de classification ascendante hiérarchique (HAC pour *Hierarchical Agglomerative Classification*) grâce à un dendogramme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3af5df-925b-4e32-974b-f86476a64517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, euclidean distance\n",
    "Z = hierarchy.linkage(df, method='single')\n",
    "\n",
    "# plot dendrogram\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Distance')\n",
    "_ = hierarchy.dendrogram(\n",
    "    Z,\n",
    "    labels=df.index,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.7 * max(Z[:, 2])\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c0187-9bc2-4325-8cb2-2ef40d4026cb",
   "metadata": {},
   "source": [
    "### Le clustering par liaison simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b44ff7-cb60-4194-b2e4-580e26c493e7",
   "metadata": {},
   "source": [
    "![Single-linkage clustering](./figs/single-linkage_clustering.svg)\n",
    "\n",
    "Une technique assez intuitive de regroupement repose sur l’idée que clusters sont reliés si deux de leurs composants sont plus proches l’un de l’autre que de n’importe quel autre composant d’un autre cluster. Pour illustrer cela, prenons quatre clusters *A*, *B*, *C* et *D* :\n",
    "\n",
    "|       | A      | B      | C   | D |\n",
    "|-------|--------|--------|----|----|\n",
    "| **A** | 0      | **12** | 13 | 34 |\n",
    "| **B** | **12** | 0      | 16 | 28 |\n",
    "| **C** | 13     | 16     | 0  | 15 |\n",
    "| **D** | 34     | 28     | 15 | 0  |\n",
    "\n",
    "Ici, les clusters *A* et *B* sont plus proches l’un de l’autre que de n’importe quel autre cluster, aussi pouvons-nous les regrouper. L’étape suivante va demander de réactualiser le tableau des distances où l’on remarque que les clusters $(A,B)$ et *C* sont désormais les plus proches :\n",
    "\n",
    "|            | (A,B)  | C      | D  |\n",
    "|------------|--------|--------|----|\n",
    "| **(A,B)**  | 0      | **13** | 28 |\n",
    "| **C**      | **13** | 0      | 15 |\n",
    "| **D**      | 28     | 15     | 0  |\n",
    "\n",
    "Après actualisation du tableau, nous obtenons :\n",
    "\n",
    "|               | ((A,B),C) | D  |\n",
    "|---------------|-----------|----|\n",
    "| **((A,B),C)** | 0         | 15 |\n",
    "| **D**         | 15        | 0  |\n",
    "\n",
    "À la fin, le cluster unique que nous avons formé est : $(((A,B),C), D)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4077f-3b8f-489e-b138-d3b956ab4d75",
   "metadata": {},
   "source": [
    "#### Décomposition étape par étape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676bd06-4bdf-41aa-8028-d1dc46ec66b6",
   "metadata": {},
   "source": [
    "L’approche du clustering par liaison simple (ou *single-linkage clustering* en anglais) est dite ascendante (ou *bottom-up*) dans le sens où elle forme des clusters élément par élément.\n",
    "\n",
    "À chaque étape, il est déjà possible de calculer le point de jonction entre les clusters, logiquement situé à équidistance entre leurs composants les plus proches. Par exemple, à la première étape, les clusters *A* et *B* sont à une distance de 12 l’un de l’autre. Si nous traçons une droite entre les deux et que nous devions en trouver le milieu, nous diviserions 12 par deux pour en déduire que le point de jonction entre les deux clusters se trouve à 6. À la seconde étape, le point de jonction entre les clusters *(A,B)* et *C* est situé à $13 \\div 2 = 6.5$.\n",
    "\n",
    "**Remarque :** on appellera plutôt ce point de jonction un nœud.\n",
    "\n",
    "Ensuite, la partie cruciale consiste à réactualiser la matrice des distances selon la fonction de liaison exprimée par la formule :\n",
    "\n",
    "$$\n",
    "D(X, Y) = \\min_{x \\in X, y \\in Y} d(x, y)\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- *X* et *Y* sont des clusters ;\n",
    "- *x* et *y* des composants de ces clusters.\n",
    "\n",
    "En reprenant notre exemple, et après avoir identifié que *A* et *B* étaient les clusters les plus proches et les avoir regroupés, nous devons déterminer lequel des deux était le plus proche de *C* et de *D* :\n",
    "\n",
    "- la distance *AC* est de 13, quand *BC* est de 16, aussi considère-t-on que le cluster $(A,B)$ se situe à une distance de 13 de *C* ;\n",
    "- $D((A,B),D) = \\min{(D(A,D), D(B,D))} = \\min{(34,28)} = 28$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86cb9e-63f1-4ab3-92e6-b89b125e219f",
   "metadata": {},
   "source": [
    "#### Algorithme de résolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3a29f-81b0-4d4a-9466-9d9fe2c91562",
   "metadata": {},
   "source": [
    "Si l’on devait concevoir à la main un programme qui effectue un clustering par liaison simple, nous répertorerions les actions suivantes à mener :\n",
    "\n",
    "- calculer la distance entre les clusters ;\n",
    "- trouver les points les plus proches entre tous les clusters ;\n",
    "- déterminer entre tous les clusters qui sont les plus proches.\n",
    "\n",
    "Définissons les fonctions nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b72c2-1096-472e-97fc-4760c900cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_linkage(k:list, l:list, m) -> float:\n",
    "    \"\"\"Calculate the minimum distance between clusters,\n",
    "    according to the single-linkage method:\n",
    "    d(k, l) = min d(u, v)\n",
    "\n",
    "    Args:\n",
    "        k -- a cluster (collection of points)\n",
    "        l -- another cluster\n",
    "        m -- matrix\n",
    "\n",
    "    Usage example:\n",
    "        min_distance = single_linkage([0,3], [6,2,9], m)\n",
    "    \"\"\"\n",
    "    return min(np.linalg.norm(m[p_k] - m[p_l]) for p_k in k for p_l in l)\n",
    "\n",
    "def points_in_cluster(cluster):\n",
    "    \"\"\"A cluster is a collection of points.\"\"\"\n",
    "    return [int(point) for point in cluster.split(',')]\n",
    "\n",
    "def dist_between_clusters(n, clusters, m):\n",
    "    return [\n",
    "        0 if n == cluster else single_linkage(points_in_cluster(n), points_in_cluster(cluster), m)\n",
    "        for cluster in clusters.loc[n].index\n",
    "    ]\n",
    "\n",
    "def nearest_clusters(m):\n",
    "    \"\"\"Find the nearest clusters in a distance matrix.\"\"\"\n",
    "\n",
    "    # looking for the minimal distance\n",
    "    min_dist = m[m > 0].min().min()\n",
    "\n",
    "    # which clusters are concerned?\n",
    "    c_1, c_2 = np.where(m == min_dist)[0]\n",
    "\n",
    "    # and their names?\n",
    "    c_1_name = m.iloc[c_1].name\n",
    "    c_2_name = m.iloc[c_2].name\n",
    "\n",
    "    return (c_1_name, c_2_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ebf69-dd5c-45c0-8127-0846041c980b",
   "metadata": {},
   "source": [
    "**1e étape :** calculer une matrice des distances où au départ chaque point est un cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf32910-5626-488f-8235-f96ac1528740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a copy of the distance matrix\n",
    "cluster_df = pd.DataFrame(distance_df.values, index=[str(i) for i in range(len(distance_df))], columns=[str(i) for i in range(len(distance_df))])\n",
    "\n",
    "# how many clusters at the end?\n",
    "n = 2\n",
    "\n",
    "# how many steps in total?\n",
    "n_steps = len(cluster_df) - n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a504c-beff-460e-b517-4a7da878204d",
   "metadata": {},
   "source": [
    "**2e étape :** trouver les clusters les plus proches et les regrouper (à répéter autant de fois que nécessaire)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822e37e-cc4e-48df-bdc4-58ecab085547",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_steps):\n",
    "\n",
    "    # find the nearest clusters\n",
    "    c_1, c_2 = nearest_clusters(cluster_df)\n",
    "    new_cluster = ','.join([c_1, c_2])\n",
    "\n",
    "    # new cluster in town\n",
    "    cluster_df.loc[new_cluster] = np.zeros(len(cluster_df))\n",
    "    cluster_df[new_cluster] = np.zeros(len(cluster_df))\n",
    "\n",
    "    # update distance matrix\n",
    "    cluster_df.loc[new_cluster] = dist_between_clusters(new_cluster, cluster_df, distance_df.values)\n",
    "    cluster_df[new_cluster] = dist_between_clusters(new_cluster, cluster_df, distance_df.values)\n",
    "\n",
    "    # delete merged clusters\n",
    "    cluster_df.drop([c_1, c_2], axis=0, inplace=True)\n",
    "    cluster_df.drop([c_1, c_2], axis=1, inplace=True)\n",
    "\n",
    "    # result\n",
    "    print(f\"Step {i + 1}: {','.join(str(int(x) + 1) for x in new_cluster.split(','))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1ae57-43b0-466a-b116-426902e8b2b2",
   "metadata": {},
   "source": [
    "### Le clustering par liaison complète"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98805c04-38a2-460c-96f7-e77154fa2775",
   "metadata": {},
   "source": [
    "![Complete-linkage clustering](./figs/complete-linkage_clustering.svg)\n",
    "\n",
    "Autre technique de partitionnement hiérarchique, le clustering par liaison complète (ou *complete-linkage clustering* en anglais) ne va pas considérer, contrairement au clustering par liaison simple, que la distance des points les plus proches de deux clusters définisse la distance entre les clusters, mais que ce soit plutôt la distance des points les plus éloignés.\n",
    "\n",
    "La fonction de laison s’exprime désormais par l’expression mathématique :\n",
    "\n",
    "$$\n",
    "D(X, Y) = \\max_{x \\in X, y \\in Y} d(x, y)\n",
    "$$\n",
    "\n",
    "En reprenant l’exemple plus haut, à la 1e étape les clusters *A* et *B* sont toujours les plus proches :\n",
    "\n",
    "|       | A      | B      | C   | D |\n",
    "|-------|--------|--------|----|----|\n",
    "| **A** | 0      | **12** | 13 | 34 |\n",
    "| **B** | **12** | 0      | 16 | 28 |\n",
    "| **C** | 13     | 16     | 0  | 15 |\n",
    "| **D** | 34     | 28     | 15 | 0  |\n",
    "\n",
    "Après réactualisation du tableau des distances avec la fonction de laison complète, nous obtenons :\n",
    "\n",
    "|            | (A,B) | C      | D      |\n",
    "|------------|-------|--------|--------|\n",
    "| **(A,B)**  | 0     | 16     | 34     |\n",
    "| **C**      | 16    | 0      | **15** |\n",
    "| **D**      | 34    | **15** | 0      |\n",
    "\n",
    "Avec cette technique, le cluster $(A,B)$ s’est éloigné de *C* et de *D*, si bien que ces deux derniers sont désormais les plus proches l’un de l’autre :\n",
    "\n",
    "|               | (A,B) | (C,D) |\n",
    "|---------------|-------|-------|\n",
    "| **(A,B)**     | 0     | 34    |\n",
    "| **(C,D)**     | 34    | 0     |\n",
    "\n",
    "À la fin, le cluster unique que nous avons formé est : $((A,B), (C,D))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f46b2d-8469-4370-9a9d-9a2274584fed",
   "metadata": {},
   "source": [
    "#### Décomposition par étapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b456f-e11e-40d6-b1a0-a14013310b0d",
   "metadata": {},
   "source": [
    "À chaque étape, il convient de regrouper d’abord deux clusters en fonction de leur proximité puis de recalculer la distance qui les sépare des autres en prenant le point le plus éloigné.\n",
    "\n",
    "Ainsi, après avoir identifié dans notre exemple que *A* et *B* étaient les clusters les plus proches et les avoir regroupés, nous devons déterminer lequel des deux était le plus **éloigné** de *C* et de *D* :\n",
    "\n",
    "- la distance *AC* est de 13, quand *BC* est de 16, aussi considère-t-on que le cluster $(A,B)$ se situe à une distance de 16 de *C* ;\n",
    "- $D((A,B),D) = \\max{(D(A,D), D(B,D))} = \\max{(34,28)} = 34$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b800e41-480b-40ed-8399-b830d8a9516a",
   "metadata": {},
   "source": [
    "#### Algorithme de résolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b2c99-354f-479b-8f34-3e6e792c6993",
   "metadata": {},
   "source": [
    "L’algorithme est pour ainsi dire identique à celui du clustering par liaison simple. Nous avons juste besoin d’une fonction spécifique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad602b40-6c7f-45cf-bbb8-241d9bc6ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_linkage(k:list, l:list, m) -> float:\n",
    "    \"\"\"Calculate the maximum distance between clusters,\n",
    "    according to the complete-linkage method:\n",
    "    d(k, l) = max d(u, v)\n",
    "\n",
    "    Args:\n",
    "        k -- a cluster (collection of points)\n",
    "        l -- another cluster\n",
    "        m -- matrix\n",
    "\n",
    "    Usage example:\n",
    "        max_distance = complete_linkage([0,3], [6,2,9], m)\n",
    "    \"\"\"\n",
    "    return max(np.linalg.norm(m[p_k] - m[p_l]) for p_k in k for p_l in l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bd8f7-172e-42aa-b816-2cb5ac4740fa",
   "metadata": {},
   "source": [
    "Et de redéfinir `dist_between_clusters()` en faisant appel à la fonction `complete_linkage()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce1cef-5b2d-48b3-913f-4ecac39c0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_between_clusters(n, clusters, m):\n",
    "    return [\n",
    "        0 if n == cluster else complete_linkage(points_in_cluster(n), points_in_cluster(cluster), m)\n",
    "        for cluster in clusters.loc[n].index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419a707-6dd4-4c96-98c8-c661853cda16",
   "metadata": {},
   "source": [
    "Pour le reste, la procédure est similaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910e323-bcb2-42a6-ba8c-e5ee9c9ced41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a copy of the distance matrix\n",
    "cluster_df = pd.DataFrame(distance_df.values, index=[str(i) for i in range(len(distance_df))], columns=[str(i) for i in range(len(distance_df))])\n",
    "\n",
    "# how many clusters at the end?\n",
    "n = 3\n",
    "\n",
    "# how many steps in total?\n",
    "n_steps = len(cluster_df) - n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc976e13-f732-428f-baf0-03855edefbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_steps):\n",
    "\n",
    "    # find the nearest clusters\n",
    "    c_1, c_2 = nearest_clusters(cluster_df)\n",
    "    new_cluster = ','.join([c_1, c_2])\n",
    "\n",
    "    # new cluster in town\n",
    "    cluster_df.loc[new_cluster] = np.zeros(len(cluster_df))\n",
    "    cluster_df[new_cluster] = np.zeros(len(cluster_df))\n",
    "\n",
    "    # update distance matrix\n",
    "    cluster_df.loc[new_cluster] = dist_between_clusters(new_cluster, cluster_df, distance_df.values)\n",
    "    cluster_df[new_cluster] = dist_between_clusters(new_cluster, cluster_df, distance_df.values)\n",
    "\n",
    "    # delete merged clusters\n",
    "    cluster_df.drop([c_1, c_2], axis=0, inplace=True)\n",
    "    cluster_df.drop([c_1, c_2], axis=1, inplace=True)\n",
    "\n",
    "    # result\n",
    "    print(f\"Step {i + 1}: {','.join(str(int(x) + 1) for x in new_cluster.split(','))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2b0ce-2975-4a1f-a0aa-01dd905f667f",
   "metadata": {},
   "source": [
    "### Le clustering par liaison centroïde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5f934-4d15-4d85-a1f3-c640c27176b3",
   "metadata": {},
   "source": [
    "![Clustering par liaison centroïde](./figs/centroid-linkage_clustering.svg)\n",
    "\n",
    "Dans cette relation, la distance entre deux clusters est la norme au carré du vecteur différence de leurs centroïdes, un centroïde étant simplement la moyenne d’une coordonnée :\n",
    "\n",
    "$$\n",
    "D(X, Y) = \\| \\mu_X - \\mu_Y \\|^2\n",
    "$$\n",
    "\n",
    "Cette méthode est un peu plus délicate que les précédentes dans la mesure où elle considère la distance entre des centroïdes et pas seulement entre des points.\n",
    "\n",
    "**Remarque :** afin de marquer plus nettement la séparation entre les clusters, la formule de la distance euclidienne est élevée au carré.\n",
    "\n",
    "Considérons cinq points dans un espace en deux dimensions et leur matrice des distances, au carré cette fois-ci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757558d-81a5-4848-b38c-f094496335fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.rand(5, 2) * 10\n",
    "distances = pdist(points) ** 2\n",
    "distance_matrix = squareform(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1f07d-aa2b-48e0-94ad-75728601332f",
   "metadata": {},
   "source": [
    "Trouvons d’abord la plus petite distance et révélons l’indice des clusters concernés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150c51b-b279-4901-985b-621e5432f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the minimal distance\n",
    "min_dist = distance_matrix[distance_matrix > 0].min().min()\n",
    "\n",
    "# which clusters are concerned?\n",
    "c_1, c_2 = np.where(distance_matrix == min_dist)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db322bd-ac42-4923-b19e-5a1f0b092178",
   "metadata": {},
   "source": [
    "Maintenant que les clusters `c_1` et `c_2` ont été identifiés comme les plus proches, il nous faut calculer leur centroïde :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46456b92-8547-4f6c-8234-bcd127c656b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = (points[c_1] + points[c_2]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72b1ec-826e-45be-8bca-79f939fad661",
   "metadata": {},
   "source": [
    "Ce centroïde devient un nouveau point dans l’espace en remplacement des points derrière `c_1` et `c_2` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd08e01-2be2-4220-903c-f63f2aef087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.delete(points, [c_1, c_2], axis=0)\n",
    "points = np.vstack([points, centroid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3d11e-9a34-439f-a8b8-699affb5c269",
   "metadata": {},
   "source": [
    "Enfin, on recalcule la matrice des distances :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a719d1-6f5d-457d-a2e7-bd19728b725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pdist(points) ** 2\n",
    "distance_matrix = squareform(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65b15e-bc71-4288-a9ba-363ebda27c1a",
   "metadata": {},
   "source": [
    "### Autres HAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abd140-02c2-4521-bfdf-536a85218ef4",
   "metadata": {},
   "source": [
    "Parmi toutes les techniques de partitionnement hiérarchique ascendant, citons encore **le clustering par liens médians** qui calcule la distance médiane entre les points des clusters, ou encore **le clustering par liaisons moyennes**, qui établit la moyenne entre toutes les paires de distances dans les clusters :\n",
    "\n",
    "![Average-linkage clustering](./figs/average-linkage_clustering.svg)\n",
    "\n",
    "Et, pour la dernière, nous avons réservé **la méthode de Ward** qui minimise la somme des carrés des écarts au sein des clusters en fusionnant les paires de clusters qui entraînent la plus petite augmentation de la variance totale.\n",
    "\n",
    "Pour toutes, le module `scipy.cluster.hierarchy` se charge des calculs grâce à une méthode `.linkage()` qui accepte un argument `method` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a38643-1df6-42b2-9e49-6ffe377d0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ward's linkage clustering\n",
    "Z = hierarchy.linkage(df, method='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72102a8-0b9e-4ad6-838d-4c1372ea8c45",
   "metadata": {},
   "source": [
    "Le module dispose également d’une méthode `.dendogram()` pour afficher le résultat du regroupement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5306bb9-f9ce-4f48-a1d6-7e1c0f828c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = hierarchy.dendrogram(\n",
    "    Z,\n",
    "    labels=df.index,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.7 * max(Z[:, 2])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd15f9-083a-4508-b72b-e686019880d8",
   "metadata": {},
   "source": [
    "## Méthodes de partitionnement non-hiérarchiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaeea7f-f64f-4718-87d9-30f2d649651b",
   "metadata": {},
   "source": [
    "Si les techniques HAC reposent sur l’idée de regrouper progressivement les éléments proches dans un espace vectoriel, les méthodes de partitionnement non-hiérarchiques ne vont pas fonder de préjugés sur la ressemblance entre les données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f1c8a-db03-416e-ac14-bb440809c36f",
   "metadata": {},
   "source": [
    "### L’algorithme des *k*-moyennes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce6cd5-e4f2-4809-ac5b-f83053233fd3",
   "metadata": {},
   "source": [
    "Il s’agit de la méthode de partitionnement la plus courante en analyse des données. Elle consiste à délimiter *k* clusters à la double condition qu’un cluster soit constitué au moins d’une donnée et que chaque donnée appartienne à un seul cluster. Rien de nouveau dans la formulation, mais la construction des clusters n’est cette fois-ci pas aussi rigide qu’avec les techniques HAC : lorsqu’un cluster est constitué, il peut à l’étape suivante se scinder ou fusionner avec un autre.\n",
    "\n",
    "On considère ces étapes :\n",
    "\n",
    "1. On détermine arbitrairement un nombre *k* de centroïdes ;\n",
    "2. les données sont assignées au cluster le plus proche ;\n",
    "3. les centroïdes sont recalculés ;\n",
    "4. il convient enfin de répéter les étapes 2 et 3 tant que nécessaire.\n",
    "\n",
    "![*k*-means clustering](./figs/kmeans.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25faf5ac-b4fc-48dd-a39a-f1d27916b430",
   "metadata": {},
   "source": [
    "#### Préparer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f3022-42e7-423a-be10-f8413454e574",
   "metadata": {},
   "source": [
    "Les algorithmes contenus dans les bibliothèques spécialisées nécessitent presque toujours de normaliser les données avant de les injecter dans les fonctions statistiques. Pour rappel, l’opération consiste, pour chaque variable, à retirer à une observation la moyenne puis à diviser par l’écart-type, selon l’expression :\n",
    "\n",
    "$$\n",
    "Z = \\frac{X − \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Avec *Numpy* et *Pandas*, on peut procéder en une seule passe si on n’omet pas le paramètre `axis=0` qui opère colonne par colonne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a0c80-af10-4ee3-b380-ee32eac8c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = (df.values - np.mean(df.values, axis=0)) / np.std(df.values, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d83f02-d46b-4436-b0c9-827f5854545f",
   "metadata": {},
   "source": [
    "#### Déterminer le nombre initial de centroïdes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405623c-f75f-438f-8906-8e1f50a8e548",
   "metadata": {},
   "source": [
    "Comme l’assignation d’un point à un cluster est au départ aléatoire, que le nombre de centroïdes est arbitraire et que l’algorithme ne garantit pas la meilleure classification possible, il est nécessaire d’effectuer plusieurs tests et d’évaluer leurs performances avant de figer les paramètres du modèle.\n",
    "\n",
    "Concernant le nombre de centroïdes, plutôt que de partir à l’aveuglette, on peut le décider à partir d’un diagramme d’éboulis qui permet de visualiser l’évolution de l’inertie (somme des distances des points au centre des clusters) en fonction du nombre de clusters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5d5d9-ee3a-45de-824e-c3c86c29504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance matrix\n",
    "distances = pdist(df_scaled, metric='euclidean')\n",
    "distance_matrix = squareform(distances)\n",
    "\n",
    "# range of clusters to try\n",
    "n_clusters_range = range(1, 11)\n",
    "\n",
    "# evolution of inertia\n",
    "inertia = []\n",
    "\n",
    "# calculate inertia for each nb of clusters\n",
    "for n_clusters in n_clusters_range:\n",
    "    centroids, _ = kmeans(df_scaled, n_clusters)\n",
    "    _, distortion = vq(df_scaled, centroids)\n",
    "    inertia.append(distortion.sum())\n",
    "\n",
    "plt.plot(n_clusters_range, inertia, marker='o', linestyle='-', markersize=8, linewidth=2)\n",
    "plt.xlabel('Number of clusters', fontsize=12)\n",
    "plt.ylabel('Inertia', fontsize=12)\n",
    "plt.title('Elbow method for optimal number of clusters', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c9546-866a-42b3-a23a-58a55e436ca7",
   "metadata": {},
   "source": [
    "#### Interpréter les clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42797ade-fe2a-4d65-a02c-0dfaf2bf33f1",
   "metadata": {},
   "source": [
    "La fonction `kmeans()` de *Scipy* permet ensuuite d’appliquer l’algorithme des *k*-moyennes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d308a-2def-449d-b88d-bab14a854f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "\n",
    "centroids, _ = kmeans(df_scaled, n_clusters)\n",
    "clusters, _ = vq(df_scaled, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131df150-2435-4dda-b72f-e6f70181a3f2",
   "metadata": {},
   "source": [
    "La variable `clusters` enregistre désormais les étiquettes des différents regroupements, tant et si bien qu’il est possible de la réinjecter dans le *data frame* pour une meilleure lisibilité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb4318-4ef0-465a-ad51-332a382d34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = clusters\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c98179-950e-4edf-bc1c-2848e2c15ab0",
   "metadata": {},
   "source": [
    "L’un des premiers marqueurs du partitionnement serait de compter le nombre d’observations dans un cluster afin de révéler des déséquilibres dans la distribution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d235e18-3764-4127-9279-9e1b532079cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848494aa-622a-447b-ae71-a880403c2f22",
   "metadata": {},
   "source": [
    "La variable `centroids` permet quant à elle de visualiser les centres des clusters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9269cf-457a-4518-9a5c-80492375159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e3211-9e9a-4291-bee5-67bf33b44f89",
   "metadata": {},
   "source": [
    "Chaque vecteur ligne de la matrice représente un cluster et chaque vecteur colonne une variable (Sciences, Politique, Littérature…). Les valeurs positives et négatives indiquent quelle variable est dominante pour tel ou tel cluster afin de révéler les influences. Si par exemple le cluster n°2 dispose de valeurs fortes en Philosophie et Littérature, cela signifie que ces catégories ont eu beaucoup d’importance lors du calcul du centroïde. En opérant ainsi cluster par cluster, on peut révéler les catégories qui les différencient.\n",
    "\n",
    "Visuellement, l’analyse sera facilitée avec une carte de chaleur (*heatmap*) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b2860-5996-49f6-8b0e-a25592217eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids into a dataframe for better visualization\n",
    "columns = df.columns[:-1] # cause we've just added another column 'Cluster'\n",
    "centroids_df = pd.DataFrame(centroids, columns=columns)\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(centroids_df, annot=True, cmap=\"coolwarm\", cbar=True, linewidths=0.5)\n",
    "plt.title(\"Heatmap\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3a821-b76b-46b6-a8b9-f84e21ca6e3b",
   "metadata": {},
   "source": [
    "Il n’est pas toujours simple de tirer des conclusions d’un partitionnement des données. Si l’on relève des déséquilibres dans la distribution des observations ou si une trop forte influence d’une caractéristique sur un cluster, cela peut simplement venir de la nature même des données. Dans les autres cas, on peut toutefois émettre des hypothèses :\n",
    "\n",
    "- Des clusters déséquilibrés suggèrent de revoir le nombre de clusters à la hausse ;\n",
    "- des clusters très petits peuvent révéler l’existence de données aberrantes qui faussent le partitionnement ;\n",
    "- des caractéristiques trop fortes indiquent sans doute un biais à évaluer.\n",
    "\n",
    "La solution n’est pas plus simple à trouver. Bien souvent, il faudra multiplier les techniques afin d’obtenir les meilleurs résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea7328-9586-40f8-92f7-f7f4b3b21eff",
   "metadata": {},
   "source": [
    "## Évaluer la qualité d’une partition d’un ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117f63b-afde-4f21-bc96-c7be69f8e331",
   "metadata": {},
   "source": [
    "### Le coefficient de silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bcbf01-eabb-4485-8476-b239e86dc11c",
   "metadata": {},
   "source": [
    "Pour évaluer si un point donné a correctement été attribué à son cluster, on utilise ordinairement une mesure appelée **coefficient de silhouette**, qui se calcule grâce à l’expression :\n",
    "\n",
    "$$\n",
    "S(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $a(i)$ est la distance moyenne entre le point *i* et tous les autres points du même cluster ;\n",
    "- $b(i)$ est la distance moyenne entre le point *i* et tous les points du cluster le plus proche (autre que le sien).\n",
    "\n",
    "Prenons quatre points situés dans un espace bidimensionnel pour lesquels nous avons déjà effectué un partitionnement :\n",
    "\n",
    "||x|y|cluster|\n",
    "|-|-|-|:-:|\n",
    "|**A**|1|2|1|\n",
    "|**B**|2|3|2|\n",
    "|**C**|8|9|2|\n",
    "|**D**|9|8|2|\n",
    "\n",
    "Nous souhaitons savoir si le point *B* a correctement été assigné au cluster 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc0dc4-5c3d-4456-84ff-4c22250da747",
   "metadata": {},
   "source": [
    "#### 1e étape : calcul de la distance moyenne de *B* dans son propre cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
