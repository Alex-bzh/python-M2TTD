{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d410f858-16c1-4925-8474-8fd68566c9be",
   "metadata": {},
   "source": [
    "# L’analyse discriminante linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c070b4e-2629-442f-a29b-d9af65595f84",
   "metadata": {},
   "source": [
    "Dans le chapitre précédent, nous avons abordé les techniques de regroupement d’observations dans des partitions (*clusters*) à des fins de classification. L’analyse discriminante linéaire (LDA pour *Linear Discriminant Analysis*) intervient en quelque sorte après, dans le sens où elle s’attache à repérer les structures qui ont autorisé l’attribution des classes (groupes).\n",
    "\n",
    "Avant d’opérer une LDA, il faut s’assurer de respecter plusieurs contraintes :\n",
    "\n",
    "- Le jeu de données est complet ;\n",
    "- toutes les variables indépendantes sont continues (c’est-à-dire normalisées) ;\n",
    "- les classes ont correctement été assignées.\n",
    "\n",
    "Par ailleurs, une LDA repose sur deux hypothèses fondamentales :\n",
    "\n",
    "- **La multinormalité :** les variables indépendantes sont normalement distribuées au sein des classes.\n",
    "- **L’homoscédasticité :** les matrices de covariance des classes sont identiques.\n",
    "\n",
    "Ajoutons à cela que la LDA cherche à maximiser la variance inter-classes en minimisant la variance intra-classe, aboutissant à une séparation linéaire entre les classes. De fait, si les données ne sont pas séparables linéairement, la LDA risque de ne pas bien fonctionner.\n",
    "\n",
    "De nombreuses contraintes, qui ne doivent malgré tout pas empêcher d’utiliser cette technique : la LDA peut s’effectuer sur des données multi-classes et ne demande pas d’énormes calculs contrairement à d’autres comme la régression logistique pour des résultats équivalents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e46410-26cc-4195-9e06-56f1cd87ce31",
   "metadata": {},
   "source": [
    "## Présentation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800573a-5f43-458a-99d3-225cfb8962cf",
   "metadata": {},
   "source": [
    "Commençons par charger toutes les bibliothèques nécessaires pour ce projet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc765c-852e-46ae-b405-5da6310cf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa182f0-8ec0-4b40-9c47-f479c1229b03",
   "metadata": {},
   "source": [
    "Prenons ensuite un jeu de données réduit issu d’une courte analyse de neuf fichiers appartenant au corpus *Brown*. L’objectif est de déterminer la meilleure manière d’analyser les données pour attribuer à coup sûr la bonne catégorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4624b-1642-42f2-8382-d9744053b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Tokens': [2374, 2370, 2428, 2213, 2334, 2332, 2200, 2234, 2244],\n",
    "    'Types': [745, 672, 737, 743, 735, 833, 956, 916, 908],\n",
    "    'TTR': [0.313816, 0.283544, 0.303542, 0.335743, 0.31491, 0.357204, 0.434545, 0.410027, 0.404635],\n",
    "    'Sents': [150, 147, 189, 84, 67, 84, 103, 95, 96],\n",
    "    'Average sentence length': [15.8267, 16.1224, 12.8466, 26.3452, 34.8358, 27.7619, 21.3592, 23.5158, 23.375],\n",
    "    '1-letter words': [416, 398, 361, 227, 262, 286, 213, 210, 265],\n",
    "    '2-letter words': [430, 432, 424, 451, 559, 384, 334, 372, 371],\n",
    "    '>15-letter words': [1, 2, 0, 1, 9, 3, 3, 6, 2],\n",
    "    'Average word length': [3.68618, 3.66624, 3.63715, 4.26661, 4.39674, 4.58533, 4.75455, 4.61549, 4.29055],\n",
    "    'Category': ['mystery', 'mystery', 'mystery', 'religion', 'religion', 'religion', 'editorial', 'editorial', 'editorial']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=['cl01', 'cl02', 'cl03', 'cd01', 'cd02', 'cd03', 'cb01', 'cb02', 'cb03'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58dcec2-3e27-47d1-9eed-e35a26d61f2f",
   "metadata": {},
   "source": [
    "## Étape 1 : choisir les variables prédictives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1cd19-1db9-4332-b414-eff13eca34d0",
   "metadata": {},
   "source": [
    "La première étape consiste à séparer les variables prédictives (celles qui nous aideront dans nos prédictions) de la variable dépendante (celle que l’on cherche à prédire). Nous identifions par ailleurs des variables redondantes : l’indice TTR étant un ratio entre les mots-formes uniques et le nombre de tokens, on peut retirer sans perte de notre modèle les variables *Tokens* et *Types*. Même traitement pour la variable *Sents* qui, à partir du moment où nous connaissons le nombre de tokens et la longueur moyenne d’une phrase, peut-être reconstituée.\n",
    "\n",
    "Nos matrices deviennent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee5ca0-180f-49e3-98e4-fe63a7193b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictive variables\n",
    "X = df.drop(columns=['Tokens', 'Types', 'Sents', 'Category']).values\n",
    "\n",
    "# target\n",
    "y = df['Category'].values\n",
    "\n",
    "# unique classes\n",
    "classes = np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5261984-9588-43f9-8061-20b72ad00cbe",
   "metadata": {},
   "source": [
    "## Étape 2 : normaliser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b365b-ca10-4e82-ba91-3fd5e36f0d5f",
   "metadata": {},
   "source": [
    "On ne le répétera jamais assez, il est généralement recommandé de normaliser sa matrice de données : d’une part certaines variables sont sur des échelles très différentes, ce qui peut influencer l’analyse ; d’autre part les algorithmes sont plus robustes en travaillant avec des variables continues.\n",
    "\n",
    "Pour *X*, on obtient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd94345e-7234-4efe-87b2-30e4e5e7e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97a98c-7098-44bc-a09a-965e597b330a",
   "metadata": {},
   "source": [
    "## Étape 3 : calculer les moyennes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6900579-7815-489a-a34e-849bc4c3cbb4",
   "metadata": {},
   "source": [
    "Afin de déterminer les variances inter-classes et intra-classes, il est nécessaire au préalable de calculer les moyennes globales et à l’intérieur des classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915a0a3-2b09-4c8b-a5c1-1e80dea35f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = X_scaled.mean(axis=0)\n",
    "means_per_class = {cls: X_scaled[y == cls].mean(axis=0) for cls in classes}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c292b0-c928-4519-8740-de84c4a32c61",
   "metadata": {},
   "source": [
    "## Étape 4 : établir les matrices de dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d8e01-6be0-4f0c-a975-dbc0d610bc3d",
   "metadata": {},
   "source": [
    "À cette étape, notre objectif est d’une part de révéler la manière dont les données varient à l’intérieur de leur classe et d’autre part comment les moyennes varient entre elles. Deux matrices seront calculées :\n",
    "\n",
    "- la matrice de dispersion intra-classe ($S_W$), qui analyse la variance/covariance des données à l’intérieur de leur classe ;\n",
    "- la matrice de dispersion inter-classe ($S_B$), qui mesure la variance entre les moyennes des classes.\n",
    "\n",
    "Avant tout, initialisons des matrices carrées nulles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4136529-1d77-41a0-91bc-c8c5a63ba6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "S_W = np.zeros((X_scaled.shape[1], X_scaled.shape[1]))\n",
    "S_B = np.zeros((X_scaled.shape[1], X_scaled.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b371e6-be57-4760-b212-95c219b43dd5",
   "metadata": {},
   "source": [
    "### La matrice de dispersion intra-classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5fab3-85df-4a7f-a132-9027b73ace09",
   "metadata": {},
   "source": [
    "Pour mesurer la variance à l’intérieur de chaque classe, nous devons nous intéresser à la dispersion des données par rapport à la moyenne dans chaque classe. La formule établit que :\n",
    "\n",
    "$$\n",
    "S_W = \\sum_{k=1}^{K} \\sum_{i \\in C_k} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $K$ est le nombre de classes ;\n",
    "- $C_k$ représente les données dans la classe $k$ ;\n",
    "- $x_i$ est un vecteur de caractéristiques pour l’exemple $i$ dans la classe $k$ ;\n",
    "- $\\mu_k$ est la moyenne des caractéristiques dans la classe $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78aa29d-b0aa-47a5-87c6-3b9547ecff4d",
   "metadata": {},
   "source": [
    "### La matrice de dispersion inter-classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a74eeb-d79f-4af5-9007-aa211124aa46",
   "metadata": {},
   "source": [
    "Cette matrice se construit en analysant les moyennes des classes varient entre elles à partir de l’expression suivante :\n",
    "\n",
    "$$\n",
    "S_B = \\sum_{k=1}^{K} N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $N_k$ est le nombre d'éléments dans la classe $k$ ;\n",
    "- $\\mu_k$ est la moyenne des caractéristiques de la classe $k$ ;\n",
    "- $\\mu$ est la moyenne globale de toutes les classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101e468-e079-4523-af7d-7ed00175ad20",
   "metadata": {},
   "source": [
    "### Résolution des calculs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124ac97-e119-48ac-af90-2d86ae589dd2",
   "metadata": {},
   "source": [
    "Les opérations se réalisent en une passe avec *Numpy* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55675d-daff-41e0-972c-90bdbf312add",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in classes:\n",
    "    # matrix for a class\n",
    "    X_cls = X_scaled[y == cls]\n",
    "    mu_cls = means_per_class[cls]\n",
    "    \n",
    "    # within-class scatter\n",
    "    S_W += np.dot((X_cls - mu_cls).T, (X_cls - mu_cls))\n",
    "    \n",
    "    # between-class scatter\n",
    "    n_cls = X_cls.shape[0]\n",
    "    S_B += n_cls * np.outer(mu_cls - mu, mu_cls - mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d580d49-e8f4-45a3-9c55-584a08c7377c",
   "metadata": {},
   "source": [
    "## Étape 5 : maximiser la séparation entre les classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b578c5a-afd2-4197-9004-b0d224a68ea6",
   "metadata": {},
   "source": [
    "Si notre objectif était simplement de minimiser la variance intra-classe, $S_W$ seule suffirait à l’analyse. Cependant, dans la LDA, l’objectif est double : rechercher aussi les directions discriminantes en projetant les données sur un sous-espace où la séparation des classes est maximisée. Mathématiquement, cela revient à calculer :\n",
    "\n",
    "$$\n",
    "S_W^{-1}S_B\n",
    "$$\n",
    "\n",
    "En prenant l’inverse de $S_W$, nous réduisons l’impact de la dispersion intra-classe sur la séparation entre les classes. Plus précisément, cela permet de mettre en évidence les directions de l’espace vectoriel où les classes sont le mieux séparées, tout en tenant compte de la variance intra-classe par la diminution de l’influence des directions où les classes se chevauchent le plus.\n",
    "\n",
    "Avec *Numpy*, on réalise l’opération simplement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a039cd-a39b-46f6-bc59-bbb42f908291",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_W_inv = np.linalg.inv(S_W)\n",
    "S_W_inv_S_B = np.dot(S_W_inv, S_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb3676-22a7-4641-8814-d12a0fdcc950",
   "metadata": {},
   "source": [
    "## Étape 6 : décomposer la matrice en éléments propres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d610d-0d6e-4d0c-8067-6d66e28a5bf9",
   "metadata": {},
   "source": [
    "Une fois la matrice $S_W^{-1}S_B$ obtenue, il reste à la décomposer en éléments propres afin de révéler :\n",
    "- Les **valeurs propres** (*eigenvalues*), qui quantifient la force de chaque direction discriminante.\n",
    "- Les **vecteurs propres** (*eigenvectors*), qui représentent les directions dans lesquelles la séparation des classes est maximisée.\n",
    "\n",
    "Pour rappel, la décomposition en éléments revient à exprimer une matrice carrée comme le résultat de l’équation :\n",
    "\n",
    "$$\n",
    "A = PDP^{-1}\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $P$ est la matrice des vecteurs propres ;\n",
    "- $D$ la diagonale des valeurs propres ;\n",
    "- $P^{-1}$ l’inverse de $P$ si $A$ est diagonalisable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78695c-671b-439e-ad52-29969169fd74",
   "metadata": {},
   "source": [
    "### Calculer les valeurs propres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca1f95-da76-4f62-b0fd-b269668de1e8",
   "metadata": {},
   "source": [
    "Cela revient à extraire les racines du polynôme caractéristique de la matrice :\n",
    "\n",
    "$$\n",
    "P_M(x) = \\det[M - x.I_n]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5cc56-c6a6-4a07-afb5-3f991b82f112",
   "metadata": {},
   "source": [
    "### Calculer les vecteurs propres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdd373-1cbe-4918-b161-f4c8f9f14d0f",
   "metadata": {},
   "source": [
    "Les vecteurs propres sont les vecteurs associés aux valeurs propres d’une matrice. Elles répondent au système d’équation ci-dessous où $I_n$ est la matrice identité et $\\vec{X}$ un vecteur des racines du polynôme caractéristique :\n",
    "\n",
    "$$\n",
    "(M − \\lambda I_n)\\vec{X} = \\vec{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35c72b-3ae2-41be-aea7-ad2e37e08c3b",
   "metadata": {},
   "source": [
    "### Projeter les données sur un sous-espace vectoriel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f2555-b26b-424f-bb30-8c074cdce60d",
   "metadata": {},
   "source": [
    "La méthode `.linalg.eig()` de *Numpy* effectue les calculs en une ligne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6c03c-7b51-4968-9c71-49be224e219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(S_W_inv_S_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a3ed53-1d86-4285-9a5d-578fb7e77d02",
   "metadata": {},
   "source": [
    "Il reste maintenant à sélectionner les vecteurs propres pour les valeurs propres les plus fortes. Pour cela, nous ordonnons d’abord les valeurs propres par ordre décroissant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f8ea7-2c18-4dac-9879-ffbf643d9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices to sort the array?\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "\n",
    "# sorting & swiping out the imaginary part of the number\n",
    "eigenvalues = np.real(eigenvalues[idx])\n",
    "eigenvectors = np.real(eigenvectors[:, idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac0257-5a60-4b92-a90d-c3f37a8ede6c",
   "metadata": {},
   "source": [
    "Ensuite, nous décidons de la dimensionnalité du sous-espace dans lequel nous souhaitons projeter les données, avec la contrainte qu’il peut, au maximum, avoir autant de dimensions que de classes $-1$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c91830-1a83-4643-b6b7-fd9637a4ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions (max here: 2)\n",
    "k = 2\n",
    "\n",
    "# select the k eigenvectors\n",
    "components = eigenvectors[:, :k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97eaa67-a652-4ab8-aa5d-4cc81cf8db14",
   "metadata": {},
   "source": [
    "Finalement, nous projetons les données sur le sous-espace :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c22e50-50fa-4df4-a87b-f5b312bd6757",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = np.dot(X_scaled, components)\n",
    "\n",
    "# into a dataframe\n",
    "df_projected = pd.DataFrame(X_projected, index=df.index, columns=['Component 1', 'Component 2'])\n",
    "df_projected['Class'] = y\n",
    "\n",
    "display(df_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f58c58-dba7-4d80-80b9-15196a16fc47",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b92094-80b6-408f-806f-663703cba6fa",
   "metadata": {},
   "source": [
    "Mettons que nous obtenons une nouvelle donnée pour laquelle nous souhaitons deviner la classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3f4c2-9309-410b-bbd8-26e6aa3c1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new text\n",
    "new_data = np.array([0.410961, 17.4314, 207, 498, 4, 3.72824]).reshape(1, -1)\n",
    "\n",
    "# standardization\n",
    "new_data_scaled = (new_data - new_data.mean()) / new_data.std()\n",
    "\n",
    "# projection\n",
    "new_data_projected = np.dot(new_data_scaled, components)\n",
    "\n",
    "# into a dataframe\n",
    "df_new = pd.DataFrame(new_data_projected, columns=['Component 1', 'Component 2'])\n",
    "df_new['Class'] = 'Unknown'\n",
    "\n",
    "# concatenation\n",
    "df_projected = pd.concat([df_projected, df_new], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b558b95-5221-4566-aba8-6155b648c963",
   "metadata": {},
   "source": [
    "Preuve que la LDA a bien fonctionné, nous visualisons dans un nuage de points des partitions distinctes qui nous incitent à attribuer la classe *mystery* au nouveau texte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a835e0c-3398-4bcc-8249-67206ca1f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df_projected,\n",
    "    x='Component 1',\n",
    "    y='Component 2',\n",
    "    hue='Class',\n",
    "    palette='Set2',\n",
    "    style='Class',\n",
    "    s=100,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title('Linear Discriminant Analysis (LDA)', fontsize=16)\n",
    "plt.xlabel('Component 1', fontsize=12)\n",
    "plt.ylabel('Component 2', fontsize=12)\n",
    "plt.legend(title='Class', title_fontsize='13', fontsize='11', loc='best')\n",
    "\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-1, 1)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97548b6-fae1-466a-a6c6-fb9fcc6559a4",
   "metadata": {},
   "source": [
    "## Établir un modèle discriminant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de7ecf-797d-4f37-9ee7-c38d9f8acaf0",
   "metadata": {},
   "source": [
    "Si elle s’interprète facilement, la preuve graphique n’est pas suffisante pour parler de modèle de classification. Une fois les données projetées sur un sous-espace de $k-1$ classes, nous pouvons construire un modèle basé sur les fonctions discriminantes de la LDA :\n",
    "\n",
    "$$\n",
    "g_k(x) = W_k \\cdot x + b_k\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $W_k$ figure les coefficients (ou poids) ;\n",
    "- $b_k$ le biais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c99ca-0462-47ea-8885-9e388da21106",
   "metadata": {},
   "source": [
    "### Calculer les coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e4160-c6ff-465a-a0e3-4a2d3481b428",
   "metadata": {},
   "source": [
    "La matrice des coefficients se calcule comme :\n",
    "\n",
    "$$\n",
    "W_k = S_W^{−1} \\cdot (\\mu_k − \\mu)\n",
    "$$\n",
    "\n",
    "Où :\n",
    "\n",
    "- $\\mu_k$ la moyenne de la classe $k$ ;\n",
    "- $\\mu$ la moyenne globale.\n",
    "\n",
    "Nous avons besoin de connaître en premier lieu les moyennes globale et de chaque classe dans l’espace projeté :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae80803-46b5-4d84-8da7-d8ddf6861b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall mean in the projected space\n",
    "mean_overall = np.mean(X_projected, axis=0)\n",
    "\n",
    "# mean of each class in the projected space\n",
    "means_per_class = [np.mean(X_projected[y == c], axis=0) for c in classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0e75d-2410-4f26-b3ee-b8b43bca5725",
   "metadata": {},
   "source": [
    "Ensuite, nous recalculons l’inverse de la matrice de dispersion intra-classe dans l’espace projeté. Dans ce contexte, $S_W$ se comprend comme la matrice de covariance des dimensions réduites obtenues après projection sur les vecteurs propres. Comme `X_projected` est une matrice contenant en lignes les observations et en colonnes les dimensions, il nous suffit de la transposer pour obtenir la covariance des dimensions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3e3cb-7504-4e5d-8147-6b6a21d1953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_W_projected = np.cov(X_projected.T)\n",
    "S_W_inv = np.linalg.inv(S_W_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd2264-ee55-4538-a759-ed3bf51d1f56",
   "metadata": {},
   "source": [
    "### Calculer le biais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445b15f-1dd8-4e02-a470-ffdbee4d97d4",
   "metadata": {},
   "source": [
    "Le biais est calculé par :\n",
    "\n",
    "$$\n",
    "b_k = -\\frac{1}{2} \\mu_k^T \\cdot S_W^{−1} \\cdot \\mu_k + \\log⁡(\\pi_k)\n",
    "$$\n",
    "\n",
    "Où $\\pi_k$ est le *prior* (probabilité a priori) de la classe $k$. L’existence de ce *prior* se justifie par la possibilité de déséquilibre de représentation d’une classe dans les données d’apprentissage. Pour éviter ce risque, le modèle prend en compte les connaissances a priori. De manière naïve, en présence de trois classes nous estimons le *prior* à 0.33 pour chacune.\n",
    "\n",
    "Dans la pratique, il faut considérer ce mécanisme comme une possibilité de représenter les connaissances a priori : si nous sommes certain·es qu’une classe se rencontre davantage dans la nature, il nous faudra bouger le curseur en conséquence. Et si en revanche les données sont parfaitement équilibrées, comme dans notre exemple où nous avons trois représentants de chaque classe, les *priors* n’auront aucun impact et pourraient être ignorées. Nous tenons toutefois à les conserver :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f30461-6e26-42db-8462-c99d01e7dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [np.sum(y == c) / len(y) for c in classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c75d1-d8c0-4990-8d23-fbf15d4dfd6f",
   "metadata": {},
   "source": [
    "### Construire le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8f058-8a72-4cd7-be84-9fd1e370bb2f",
   "metadata": {},
   "source": [
    "Tous les éléments étant en place, nous pouvons construire le modèle en reprenant la formule énoncée plus haut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4376f-a2ad-439c-864f-804fdd53ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = []\n",
    "for k, class_mean in enumerate(means_per_class):\n",
    "    W_k = np.dot(S_W_inv, class_mean - mean_overall)\n",
    "    b_k = -0.5 * np.dot(class_mean, np.dot(S_W_inv, class_mean)) + np.log(priors[k])\n",
    "    coefficients.append((W_k, b_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4db97-f31e-4546-bdbc-e0dbd4cccfc4",
   "metadata": {},
   "source": [
    "### Effectuer des prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf567d-7fae-458a-95f0-a2425e419973",
   "metadata": {},
   "source": [
    "Il est temps d’utiliser le modèle pour prédire la classe de notre nouvelle observation, selon l'expression :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_k g_k(x)\n",
    "$$\n",
    "\n",
    "La formule se comprend facilement : nous appliquons la fonction discriminante du modèle ajusté à chaque classe et choisissons la classe correspondant à l’argument qui maximise la fonction discriminante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae672c4-202a-4656-bafc-d93e3f4687e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [\n",
    "    np.dot(new_data_projected, W_k) + b_k\n",
    "    for W_k, b_k in coefficients\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Score : {np.max(scores):.4f}\",\n",
    "    f\"Catégorie : {classes[np.argmax(scores)]}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d81ab-d48f-4dfd-8516-c367a7431350",
   "metadata": {},
   "source": [
    "Cerise sur le gâteau, il est possible de ressortir la probabilité associée à chaque classe grâce à la fonction de densité a posteriori :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6c463-7c52-4479-a2f7-19c8464741a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to Bayes theorem\n",
    "exps = np.exp(scores - np.max(scores))\n",
    "probabilities = exps / np.sum(exps)\n",
    "\n",
    "for class_name, p in zip(classes, probabilities):\n",
    "    print(f'{class_name} : {p[0]:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515bbe1-8b09-486a-84e7-4073403ff3c5",
   "metadata": {},
   "source": [
    "## Évaluer la performance du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e1ecb-365f-435a-b718-79af373b5c77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
